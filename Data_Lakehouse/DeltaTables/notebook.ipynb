{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Delta Tables in Fabric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring OptimizeWrite Function for the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Optimize Write at the Spark session level\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", False)\n",
    "\n",
    "# Enable Optimize Write at the Spark session level\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", True)\n",
    "\n",
    "print(spark.conf.get(\"spark.microsoft.delta.optimizeWrite.enabled\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting V-Order functionality for the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.parquet.vorder.enabled', 'true')\n",
    "\n",
    "print(spark.conf.get('spark.sql.parquet.vorder.enabled'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading 2019.csv and loading it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Files/2019.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying 2019.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the schema for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "orderSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    StructField(\"OrderDate\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"Item\", StringType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"UnitPrice\", FloatType()),\n",
    "    StructField(\"Tax\", FloatType())\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(orderSchema).load(\"Files/2019.csv\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Managed Table from the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.write.format(\"delta\").saveAsTable(\"Sales_Managed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an External Table from the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").saveAsTable(\"Sales_External\", path=\"abfs_path/Sales_External\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the External and Managed Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " DESCRIBE FORMATTED sales_managed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " DESCRIBE FORMATTED sales_external;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DROP both the Tables to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " DROP TABLE sales_managed;\n",
    " DROP TABLE sales_external;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SQL to create a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " CREATE TABLE sales_external\n",
    " USING DELTA\n",
    " LOCATION 'Files/Sales_External';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM sales_external;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Table Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "UPDATE sales_external\n",
    "SET Quantity = 5\n",
    "WHERE Item = 'Mountain-100 Silver, 44';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " DESCRIBE HISTORY sales_external;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the \"VACUUM\" Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "VACUUM Sales_External"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning Data by \"Item\" field and then creating an external \"sales_partitioned\" table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning Delta Table by “Category”\n",
    "df.write.format(\"delta\").partitionBy(\"Item\").saveAsTable(\"sales_partitioned\", path=\"abfs_path/partitioned_products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Data with Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a folder\n",
    "inputPath = 'Files/data/'\n",
    "mssparkutils.fs.mkdirs(inputPath)\n",
    "\n",
    "# Create a stream that reads data from the folder, using a JSON schema\n",
    "jsonSchema = StructType([\n",
    "StructField(\"device\", StringType(), False),\n",
    "StructField(\"status\", StringType(), False)\n",
    "])\n",
    "iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
    "\n",
    "# Write some event data to the folder\n",
    "device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"ok\"}\n",
    "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
    "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
    "\n",
    "mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\n",
    "\n",
    "print(\"Source stream created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Write the stream to a delta table\n",
    " delta_stream_table_path = 'Tables/iotdevicedata'\n",
    " checkpointpath = 'Files/delta/checkpoint'\n",
    " deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\n",
    " print(\"Streaming to delta sink...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " SELECT * FROM IotDeviceData;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%sql\n",
    " SELECT COUNT(*) FROM IotDeviceData;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Add more data to the source stream\n",
    " more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    " {\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    " {\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    " {\"device\":\"Dev1\",\"status\":\"ok\"}\n",
    " {\"device\":\"Dev1\",\"status\":\"error\"}\n",
    " {\"device\":\"Dev2\",\"status\":\"error\"}\n",
    " {\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
    "\n",
    " mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) FROM IotDeviceData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " deltastream.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
